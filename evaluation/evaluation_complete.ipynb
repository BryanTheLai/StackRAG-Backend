{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9716a55",
   "metadata": {},
   "source": [
    "# AI CFO - Complete RAG Evaluation\n",
    "\n",
    "This notebook provides a complete, working evaluation framework for the AI CFO RAG system.\n",
    "\n",
    "## Key Features:\n",
    "- âœ… **Robust Error Handling**: Catches and reports all failure modes\n",
    "- âœ… **Fallback Mechanisms**: Uses simplified evaluation when agent fails\n",
    "- âœ… **Real Context Capture**: Instruments retrieval service for actual context\n",
    "- âœ… **Comprehensive Metrics**: Context precision, faithfulness, answer correctness, number accuracy\n",
    "- âœ… **Clear Reporting**: Visual results with actionable insights\n",
    "\n",
    "## Evaluation Flow:\n",
    "1. **Setup & Authentication**: Configure environment and test user session\n",
    "2. **System Health Check**: Verify user has documents and chunks available\n",
    "3. **Test Execution**: Run golden dataset through both full RAG and retrieval-only modes\n",
    "4. **Metrics Calculation**: LLM-as-a-judge + numerical accuracy validation\n",
    "5. **Results Analysis**: Summary statistics, visualizations, and recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc90b0f",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648f424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import traceback\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from litellm import completion\n",
    "from supabase import create_client, Client\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"ğŸ“ Project root: {project_root}\")\n",
    "print(f\"ğŸ“ Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Import configurations and dependencies\n",
    "from evaluation.config import (\n",
    "    GOLDEN_DATASET_PATH, EVAL_RESULTS_PATH, JUDGE_MODEL,\n",
    "    CONTEXT_PRECISION_PROMPT, FAITHFULNESS_PROMPT, ANSWER_CORRECTNESS_PROMPT,\n",
    "    TEST_USER_EMAIL, TEST_USER_PASSWORD, SUPABASE_URL, SUPABASE_KEY\n",
    ")\n",
    "\n",
    "from api.v1.dependencies import Session\n",
    "from src.llm.workflow.react_rag import run_react_rag\n",
    "from src.llm.tools.FunctionCaller import RetrievalService\n",
    "from src.llm.OpenAIClient import OpenAIClient\n",
    "from src.storage.SupabaseService import SupabaseService\n",
    "\n",
    "print(\"âœ… All imports successful\")\n",
    "print(f\"ğŸ¯ Judge Model: {JUDGE_MODEL}\")\n",
    "print(f\"ğŸ“Š Golden Dataset: {GOLDEN_DATASET_PATH}\")\n",
    "print(f\"ğŸ’¾ Results Path: {EVAL_RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac7f05f",
   "metadata": {},
   "source": [
    "## 2. Authentication & Client Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994e6882",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def setup_auth_session() -> tuple[Session, Client]:\n",
    "    \"\"\"Create authenticated session and Supabase client.\"\"\"\n",
    "    print(\"ğŸ” Setting up authentication...\")\n",
    "    \n",
    "    if not all([TEST_USER_EMAIL, TEST_USER_PASSWORD, SUPABASE_URL, SUPABASE_KEY]):\n",
    "        raise ValueError(\"âŒ Missing required environment variables for authentication\")\n",
    "    \n",
    "    auth_client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "    \n",
    "    try:\n",
    "        response = await asyncio.to_thread(\n",
    "            auth_client.auth.sign_in_with_password,\n",
    "            {\"email\": TEST_USER_EMAIL, \"password\": TEST_USER_PASSWORD}\n",
    "        )\n",
    "        \n",
    "        if not response.session or not response.user:\n",
    "            raise Exception(\"Authentication failed - no session/user returned\")\n",
    "        \n",
    "        session = Session(user_id=response.user.id, token=response.session.access_token)\n",
    "        \n",
    "        # Create authenticated client\n",
    "        supabase_client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "        supabase_client.options.headers[\"Authorization\"] = f\"Bearer {session.token}\"\n",
    "        \n",
    "        print(f\"âœ… Authenticated as: {session.user_id}\")\n",
    "        return session, supabase_client\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Authentication failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Setup authentication\n",
    "session, supabase_client = await setup_auth_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcfeefa",
   "metadata": {},
   "source": [
    "## 3. System Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25582b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def check_system_health(session: Session, client: Client) -> Dict[str, Any]:\n",
    "    \"\"\"Verify the user has documents and chunks available for testing.\"\"\"\n",
    "    print(\"ğŸ¥ Running system health check...\")\n",
    "    \n",
    "    health = {\n",
    "        \"user_id\": session.user_id,\n",
    "        \"documents\": 0,\n",
    "        \"chunks\": 0,\n",
    "        \"retrieval_test\": False,\n",
    "        \"errors\": []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Check documents\n",
    "        docs_resp = client.table('documents').select('id, filename, report_date').eq('user_id', session.user_id).execute()\n",
    "        health[\"documents\"] = len(docs_resp.data)\n",
    "        \n",
    "        print(f\"ğŸ“ Found {health['documents']} documents:\")\n",
    "        for doc in docs_resp.data[:5]:  # Show first 5\n",
    "            print(f\"   - {doc['filename']} ({doc.get('report_date', 'No date')})\")\n",
    "        \n",
    "        if health[\"documents\"] == 0:\n",
    "            health[\"errors\"].append(\"No documents found for user\")\n",
    "            return health\n",
    "            \n",
    "    except Exception as e:\n",
    "        health[\"errors\"].append(f\"Error checking documents: {e}\")\n",
    "    \n",
    "    try:\n",
    "        # Check chunks - try both table names\n",
    "        for table_name in ['chunks', 'document_chunks']:\n",
    "            try:\n",
    "                chunks_resp = client.table(table_name).select('id').eq('user_id', session.user_id).execute()\n",
    "                health[\"chunks\"] = len(chunks_resp.data)\n",
    "                print(f\"ğŸ“„ Found {health['chunks']} chunks in '{table_name}' table\")\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "        if health[\"chunks\"] == 0:\n",
    "            health[\"errors\"].append(\"No document chunks found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        health[\"errors\"].append(f\"Error checking chunks: {e}\")\n",
    "    \n",
    "    try:\n",
    "        # Test retrieval service\n",
    "        retrieval = RetrievalService(\n",
    "            openai_client=OpenAIClient(),\n",
    "            supabase_service=SupabaseService(supabase_client=client),\n",
    "            user_id=session.user_id\n",
    "        )\n",
    "        \n",
    "        result = retrieval.retrieve_chunks(\"revenue 2023\", 3)\n",
    "        chunks_data = json.loads(result)\n",
    "        \n",
    "        if chunks_data and len(chunks_data) > 0:\n",
    "            health[\"retrieval_test\"] = True\n",
    "            print(f\"âœ… Retrieval test successful: {len(chunks_data)} chunks retrieved\")\n",
    "        else:\n",
    "            health[\"errors\"].append(\"Retrieval test returned no results\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        health[\"errors\"].append(f\"Retrieval test failed: {e}\")\n",
    "    \n",
    "    # Overall health assessment\n",
    "    is_healthy = (health[\"documents\"] > 0 and \n",
    "                  health[\"chunks\"] > 0 and \n",
    "                  health[\"retrieval_test\"] and \n",
    "                  len(health[\"errors\"]) == 0)\n",
    "    \n",
    "    if is_healthy:\n",
    "        print(\"ğŸŸ¢ System is healthy and ready for evaluation\")\n",
    "    else:\n",
    "        print(\"ğŸŸ¡ System has issues but evaluation will continue:\")\n",
    "        for error in health[\"errors\"]:\n",
    "            print(f\"   âš ï¸ {error}\")\n",
    "    \n",
    "    return health\n",
    "\n",
    "# Run health check\n",
    "system_health = await check_system_health(session, supabase_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f7d77c",
   "metadata": {},
   "source": [
    "## 4. Load and Validate Golden Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73ca7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_golden_dataset() -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load and validate the golden dataset.\"\"\"\n",
    "    print(f\"ğŸ“š Loading golden dataset from: {GOLDEN_DATASET_PATH}\")\n",
    "    \n",
    "    try:\n",
    "        with open(GOLDEN_DATASET_PATH, 'r', encoding='utf-8') as f:\n",
    "            dataset = json.load(f)\n",
    "        \n",
    "        # Validate structure\n",
    "        required_fields = ['test_id', 'question', 'ideal_answer', 'ground_truth_context', 'tags']\n",
    "        \n",
    "        for i, item in enumerate(dataset):\n",
    "            missing = [field for field in required_fields if field not in item]\n",
    "            if missing:\n",
    "                raise ValueError(f\"Test case {i} missing fields: {missing}\")\n",
    "        \n",
    "        print(f\"âœ… Loaded {len(dataset)} valid test cases\")\n",
    "        \n",
    "        # Show test overview\n",
    "        test_ids = [item['test_id'] for item in dataset]\n",
    "        all_tags = [tag for item in dataset for tag in item['tags']]\n",
    "        tag_counts = pd.Series(all_tags).value_counts()\n",
    "        \n",
    "        print(f\"ğŸ“‹ Test IDs: {', '.join(test_ids)}\")\n",
    "        print(f\"ğŸ·ï¸ Tag distribution: {dict(tag_counts)}\")\n",
    "        \n",
    "        return dataset\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load golden dataset\n",
    "golden_dataset = load_golden_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26689c44",
   "metadata": {},
   "source": [
    "## 5. Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dc530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstrumentedRetrievalService(RetrievalService):\n",
    "    \"\"\"Instrumented retrieval service that captures context for evaluation.\"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.last_retrieved_context = \"\"\n",
    "        \n",
    "    def retrieve_chunks(self, query: str, num_chunks: int = 10) -> str:\n",
    "        \"\"\"Override to capture context.\"\"\"\n",
    "        result = super().retrieve_chunks(query, num_chunks)\n",
    "        self.last_retrieved_context = result\n",
    "        return result\n",
    "    \n",
    "    def get_last_context(self) -> str:\n",
    "        return self.last_retrieved_context\n",
    "\n",
    "async def llm_judge(prompt: str, model: str = JUDGE_MODEL) -> str:\n",
    "    \"\"\"Call LLM judge for evaluation.\"\"\"\n",
    "    try:\n",
    "        response = completion(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.0,\n",
    "            max_tokens=10,\n",
    "        )\n",
    "        \n",
    "        content = response.choices[0].message.content.strip().upper()\n",
    "        \n",
    "        if \"YES\" in content:\n",
    "            return \"YES\"\n",
    "        elif \"NO\" in content:\n",
    "            return \"NO\"\n",
    "        else:\n",
    "            return \"UNKNOWN\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"    âŒ Judge error: {e}\")\n",
    "        return \"ERROR\"\n",
    "\n",
    "def extract_numbers(text: str) -> set:\n",
    "    \"\"\"Extract financial numbers from text.\"\"\"\n",
    "    pattern = r'[\\$â‚¬Â£]?\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?%?'\n",
    "    numbers = re.findall(pattern, text)\n",
    "    return {re.sub(r'[\\$â‚¬Â£,]', '', num) for num in numbers}\n",
    "\n",
    "def check_number_accuracy(generated: str, ideal: str) -> str:\n",
    "    \"\"\"Compare numerical accuracy.\"\"\"\n",
    "    try:\n",
    "        gen_nums = extract_numbers(generated)\n",
    "        ideal_nums = extract_numbers(ideal)\n",
    "        \n",
    "        if ideal_nums and not gen_nums:\n",
    "            return \"NO\"\n",
    "        if not ideal_nums:\n",
    "            return \"YES\"\n",
    "            \n",
    "        missing = ideal_nums - gen_nums\n",
    "        return \"NO\" if missing else \"YES\"\n",
    "        \n",
    "    except Exception:\n",
    "        return \"ERROR\"\n",
    "\n",
    "print(\"âœ… Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e5d023",
   "metadata": {},
   "source": [
    "## 6. RAG Response Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728dba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_full_rag_response(question: str, session: Session, client: Client) -> Dict[str, Any]:\n",
    "    \"\"\"Get response using the full RAG pipeline with agent orchestration.\"\"\"\n",
    "    print(f\"  ğŸ¤– Full RAG: {question[:50]}...\")\n",
    "    \n",
    "    try:\n",
    "        # Patch retrieval service for context capture\n",
    "        instrumented_retrieval = InstrumentedRetrievalService(\n",
    "            openai_client=OpenAIClient(),\n",
    "            supabase_service=SupabaseService(supabase_client=client),\n",
    "            user_id=session.user_id\n",
    "        )\n",
    "        \n",
    "        # Monkey patch for context capture\n",
    "        import src.llm.workflow.react_rag as rag_module\n",
    "        original_class = rag_module.RetrievalService\n",
    "        rag_module.RetrievalService = lambda *args, **kwargs: instrumented_retrieval\n",
    "        \n",
    "        try:\n",
    "            final_answer = \"\"\n",
    "            chunk_count = 0\n",
    "            error_encountered = False\n",
    "            \n",
    "            try:\n",
    "                async for chunk in run_react_rag(session, client, question, []):\n",
    "                    final_answer += chunk\n",
    "                    chunk_count += 1\n",
    "            except AssertionError as ae:\n",
    "                # Handle known Gemini API assertion error\n",
    "                print(f\"    âš ï¸ Gemini API assertion error (likely safety filtering): {ae}\")\n",
    "                error_encountered = True\n",
    "                final_answer = \"ERROR: Gemini API assertion error - likely content was filtered by safety settings\"\n",
    "            except Exception as stream_error:\n",
    "                print(f\"    âš ï¸ Streaming error: {stream_error}\")\n",
    "                error_encountered = True\n",
    "                final_answer = f\"ERROR: Streaming failed - {str(stream_error)}\"\n",
    "            \n",
    "            retrieved_context = instrumented_retrieval.get_last_context()\n",
    "            \n",
    "            if error_encountered:\n",
    "                print(f\"    âš ï¸ Full RAG had errors but context was retrieved: {len(retrieved_context)} context chars\")\n",
    "                return {\n",
    "                    \"method\": \"full_rag\",\n",
    "                    \"final_answer\": final_answer,\n",
    "                    \"retrieved_context\": retrieved_context,\n",
    "                    \"chunk_count\": chunk_count,\n",
    "                    \"success\": False,\n",
    "                    \"error\": \"streaming_error\"\n",
    "                }\n",
    "            else:\n",
    "                print(f\"    âœ… Full RAG complete: {len(final_answer)} chars, {len(retrieved_context)} context chars\")\n",
    "                return {\n",
    "                    \"method\": \"full_rag\",\n",
    "                    \"final_answer\": final_answer.strip(),\n",
    "                    \"retrieved_context\": retrieved_context,\n",
    "                    \"chunk_count\": chunk_count,\n",
    "                    \"success\": True\n",
    "                }\n",
    "            \n",
    "        finally:\n",
    "            rag_module.RetrievalService = original_class\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"    âŒ Full RAG failed: {e}\")\n",
    "        return {\n",
    "            \"method\": \"full_rag\",\n",
    "            \"final_answer\": f\"ERROR: {str(e)}\",\n",
    "            \"retrieved_context\": \"ERROR\",\n",
    "            \"chunk_count\": 0,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "async def get_retrieval_only_response(question: str, session: Session, client: Client) -> Dict[str, Any]:\n",
    "    \"\"\"Get response using only retrieval + simple answer generation.\"\"\"\n",
    "    print(f\"  ğŸ” Retrieval only: {question[:50]}...\")\n",
    "    \n",
    "    try:\n",
    "        retrieval = RetrievalService(\n",
    "            openai_client=OpenAIClient(),\n",
    "            supabase_service=SupabaseService(supabase_client=client),\n",
    "            user_id=session.user_id\n",
    "        )\n",
    "        \n",
    "        result = retrieval.retrieve_chunks(question, 5)\n",
    "        chunks_data = json.loads(result)\n",
    "        \n",
    "        # Generate simple answer from chunks\n",
    "        if chunks_data:\n",
    "            # Extract key data\n",
    "            financial_data = []\n",
    "            for chunk in chunks_data:\n",
    "                text = chunk.get('chunk_text', '')\n",
    "                numbers = extract_numbers(text)\n",
    "                if numbers:\n",
    "                    financial_data.extend(numbers)\n",
    "            \n",
    "            if financial_data:\n",
    "                # Create answer with found numbers\n",
    "                context_preview = chunks_data[0].get('chunk_text', '')[:200]\n",
    "                mock_answer = f\"Based on the documents, I found the following data: {', '.join(list(financial_data)[:3])}. Source: {context_preview}...\"\n",
    "            else:\n",
    "                mock_answer = f\"I found relevant documents but no specific financial figures. Context: {chunks_data[0].get('chunk_text', '')[:100]}...\"\n",
    "        else:\n",
    "            mock_answer = \"No relevant documents found for this question.\"\n",
    "        \n",
    "        print(f\"    âœ… Retrieval complete: {len(chunks_data)} chunks, {len(mock_answer)} chars\")\n",
    "        \n",
    "        return {\n",
    "            \"method\": \"retrieval_only\",\n",
    "            \"final_answer\": mock_answer,\n",
    "            \"retrieved_context\": result,\n",
    "            \"chunk_count\": len(chunks_data),\n",
    "            \"success\": True\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    âŒ Retrieval failed: {e}\")\n",
    "        return {\n",
    "            \"method\": \"retrieval_only\",\n",
    "            \"final_answer\": f\"ERROR: {str(e)}\",\n",
    "            \"retrieved_context\": \"ERROR\",\n",
    "            \"chunk_count\": 0,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "async def get_best_response(question: str, session: Session, client: Client) -> Dict[str, Any]:\n",
    "    \"\"\"Try full RAG first, fall back to retrieval-only if it fails.\"\"\"\n",
    "    # Try full RAG first\n",
    "    full_response = await get_full_rag_response(question, session, client)\n",
    "    \n",
    "    if full_response[\"success\"]:\n",
    "        return full_response\n",
    "    \n",
    "    print(f\"    ğŸ”„ Falling back to retrieval-only...\")\n",
    "    return await get_retrieval_only_response(question, session, client)\n",
    "\n",
    "print(\"âœ… RAG response functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82808ad",
   "metadata": {},
   "source": [
    "## 7. Main Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9834f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_single_test_case(test_case: Dict[str, Any], session: Session, client: Client) -> Dict[str, Any]:\n",
    "    \"\"\"Evaluate a single test case with comprehensive error handling.\"\"\"\n",
    "    test_id = test_case['test_id']\n",
    "    question = test_case['question']\n",
    "    \n",
    "    print(f\"ğŸ“ {test_id}: {question}\")\n",
    "    \n",
    "    # Get response (with fallback)\n",
    "    response = await get_best_response(question, session, client)\n",
    "    \n",
    "    generated_answer = response['final_answer']\n",
    "    retrieved_context = response['retrieved_context']\n",
    "    \n",
    "    print(f\"  ğŸ“Š Method: {response['method']}, Success: {response['success']}\")\n",
    "    \n",
    "    # Run evaluations\n",
    "    print(f\"  ğŸ” Running evaluations...\")\n",
    "    \n",
    "    # Context Precision\n",
    "    if \"ERROR\" in retrieved_context or not response['success']:\n",
    "        context_precision = \"NO\"\n",
    "    else:\n",
    "        prompt = CONTEXT_PRECISION_PROMPT.format(\n",
    "            question=question,\n",
    "            retrieved_context=retrieved_context[:1000]\n",
    "        )\n",
    "        context_precision = await llm_judge(prompt)\n",
    "    \n",
    "    # Faithfulness\n",
    "    if \"ERROR\" in retrieved_context or \"ERROR\" in generated_answer or not response['success']:\n",
    "        faithfulness = \"NO\"\n",
    "    else:\n",
    "        prompt = FAITHFULNESS_PROMPT.format(\n",
    "            retrieved_context=retrieved_context[:1000],\n",
    "            generated_answer=generated_answer\n",
    "        )\n",
    "        faithfulness = await llm_judge(prompt)\n",
    "    \n",
    "    # Answer Correctness\n",
    "    prompt = ANSWER_CORRECTNESS_PROMPT.format(\n",
    "        question=question,\n",
    "        ideal_answer=test_case['ideal_answer'],\n",
    "        generated_answer=generated_answer\n",
    "    )\n",
    "    answer_correctness = await llm_judge(prompt)\n",
    "    \n",
    "    # Number Accuracy\n",
    "    number_accuracy = check_number_accuracy(generated_answer, test_case['ideal_answer'])\n",
    "    \n",
    "    # Results summary\n",
    "    results = {\n",
    "        **test_case,\n",
    "        \"generated_answer\": generated_answer,\n",
    "        \"retrieved_context\": retrieved_context,\n",
    "        \"method_used\": response['method'],\n",
    "        \"rag_success\": response['success'],\n",
    "        \"context_precision\": context_precision,\n",
    "        \"faithfulness\": faithfulness,\n",
    "        \"answer_correctness\": answer_correctness,\n",
    "        \"number_accuracy\": number_accuracy,\n",
    "        \"response_length\": len(generated_answer),\n",
    "        \"context_length\": len(retrieved_context),\n",
    "        \"chunk_count\": response.get('chunk_count', 0)\n",
    "    }\n",
    "    \n",
    "    print(f\"  ğŸ“Š Results: CP={context_precision}, F={faithfulness}, AC={answer_correctness}, NA={number_accuracy}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "async def run_complete_evaluation() -> pd.DataFrame:\n",
    "    \"\"\"Run the complete evaluation pipeline.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸš€ STARTING COMPLETE EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, test_case in enumerate(golden_dataset):\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"ğŸ“‹ Test {i+1}/{len(golden_dataset)}: {test_case['test_id']}\")\n",
    "        print(f\"ğŸ·ï¸ Tags: {', '.join(test_case['tags'])}\")\n",
    "        print(f\"{'='*40}\")\n",
    "        \n",
    "        try:\n",
    "            result = await evaluate_single_test_case(test_case, session, supabase_client)\n",
    "            results.append(result)\n",
    "            print(f\"âœ… Test {test_case['test_id']} completed\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Test {test_case['test_id']} failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            # Create error result\n",
    "            error_result = {\n",
    "                **test_case,\n",
    "                \"generated_answer\": f\"SYSTEM_ERROR: {str(e)}\",\n",
    "                \"retrieved_context\": \"SYSTEM_ERROR\",\n",
    "                \"method_used\": \"error\",\n",
    "                \"rag_success\": False,\n",
    "                \"context_precision\": \"ERROR\",\n",
    "                \"faithfulness\": \"ERROR\",\n",
    "                \"answer_correctness\": \"ERROR\",\n",
    "                \"number_accuracy\": \"ERROR\",\n",
    "                \"response_length\": 0,\n",
    "                \"context_length\": 0,\n",
    "                \"chunk_count\": 0\n",
    "            }\n",
    "            results.append(error_result)\n",
    "        \n",
    "        # Rate limiting delay\n",
    "        if i < len(golden_dataset) - 1:\n",
    "            print(\"â³ Waiting 2 seconds...\")\n",
    "            await asyncio.sleep(2)\n",
    "    \n",
    "    # Convert to DataFrame and save\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(EVAL_RESULTS_PATH, index=False)\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ Results saved to: {EVAL_RESULTS_PATH}\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ‰ EVALUATION COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "print(\"âœ… Evaluation pipeline defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8320f5",
   "metadata": {},
   "source": [
    "## 8. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60ebfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete evaluation\n",
    "evaluation_results = await run_complete_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b733e47b",
   "metadata": {},
   "source": [
    "## 9. Results Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe99ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"Comprehensive results analysis.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“Š EVALUATION RESULTS ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Basic stats\n",
    "    total_tests = len(df)\n",
    "    successful_rag = (df['rag_success'] == True).sum()\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ Overview:\")\n",
    "    print(f\"  Total test cases: {total_tests}\")\n",
    "    print(f\"  Successful RAG calls: {successful_rag}/{total_tests} ({(successful_rag/total_tests)*100:.1f}%)\")\n",
    "    print(f\"  Average response length: {df['response_length'].mean():.0f} characters\")\n",
    "    print(f\"  Average context length: {df['context_length'].mean():.0f} characters\")\n",
    "    \n",
    "    # Method breakdown\n",
    "    method_counts = df['method_used'].value_counts()\n",
    "    print(f\"\\nğŸ”§ Methods used:\")\n",
    "    for method, count in method_counts.items():\n",
    "        print(f\"  {method}: {count}/{total_tests} ({(count/total_tests)*100:.1f}%)\")\n",
    "    \n",
    "    # Metric analysis\n",
    "    metrics = ['context_precision', 'faithfulness', 'answer_correctness', 'number_accuracy']\n",
    "    summary = {}\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Metric Performance:\")\n",
    "    for metric in metrics:\n",
    "        counts = df[metric].value_counts()\n",
    "        yes_count = counts.get('YES', 0)\n",
    "        success_rate = (yes_count / total_tests) * 100\n",
    "        \n",
    "        summary[metric] = {\n",
    "            'success_rate': success_rate,\n",
    "            'yes_count': yes_count,\n",
    "            'no_count': counts.get('NO', 0),\n",
    "            'error_count': counts.get('ERROR', 0)\n",
    "        }\n",
    "        \n",
    "        print(f\"  {metric.replace('_', ' ').title()}: {yes_count}/{total_tests} ({success_rate:.1f}%)\")\n",
    "        if counts.get('ERROR', 0) > 0:\n",
    "            print(f\"    âš ï¸ Errors: {counts.get('ERROR', 0)}\")\n",
    "    \n",
    "    # Overall score\n",
    "    overall_score = sum(s['success_rate'] for s in summary.values()) / len(summary)\n",
    "    print(f\"\\nğŸ¯ Overall Performance Score: {overall_score:.1f}%\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def create_visualizations(df: pd.DataFrame):\n",
    "    \"\"\"Create comprehensive visualizations.\"\"\"\n",
    "    print(\"\\nğŸ“ˆ Creating visualizations...\")\n",
    "    \n",
    "    # Set up plotting\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('AI CFO RAG System - Complete Evaluation Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Success rates by metric\n",
    "    ax1 = axes[0, 0]\n",
    "    metrics = ['context_precision', 'faithfulness', 'answer_correctness', 'number_accuracy']\n",
    "    metric_labels = [m.replace('_', ' ').title() for m in metrics]\n",
    "    success_rates = [(df[m] == 'YES').mean() * 100 for m in metrics]\n",
    "    \n",
    "    bars = ax1.bar(metric_labels, success_rates, color=['#2E86AB', '#A23B72', '#F18F01', '#C73E1D'])\n",
    "    ax1.set_title('Success Rates by Metric')\n",
    "    ax1.set_ylabel('Success Rate (%)')\n",
    "    ax1.set_ylim(0, 100)\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for bar, rate in zip(bars, success_rates):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{rate:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Method usage\n",
    "    ax2 = axes[0, 1]\n",
    "    method_counts = df['method_used'].value_counts()\n",
    "    colors = ['#2E86AB', '#F18F01', '#C73E1D']\n",
    "    wedges, texts, autotexts = ax2.pie(method_counts.values, labels=method_counts.index, \n",
    "                                       autopct='%1.1f%%', colors=colors[:len(method_counts)])\n",
    "    ax2.set_title('Method Usage Distribution')\n",
    "    \n",
    "    # 3. Response length distribution\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.hist(df['response_length'], bins=15, color='#2E86AB', alpha=0.7, edgecolor='black')\n",
    "    ax3.set_title('Response Length Distribution')\n",
    "    ax3.set_xlabel('Response Length (characters)')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    \n",
    "    # 4. Success correlation heatmap\n",
    "    ax4 = axes[1, 1]\n",
    "    # Convert metrics to numeric\n",
    "    numeric_df = df[metrics].copy()\n",
    "    for col in metrics:\n",
    "        numeric_df[col] = numeric_df[col].map({'YES': 1, 'NO': 0, 'ERROR': -1, 'UNKNOWN': -1})\n",
    "    \n",
    "    correlation_matrix = numeric_df.corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='RdYlBu_r', center=0, \n",
    "                square=True, ax=ax4, cbar_kws={'shrink': 0.8})\n",
    "    ax4.set_title('Metric Correlation Matrix')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_detailed_results_table(df: pd.DataFrame):\n",
    "    \"\"\"Create a styled detailed results table.\"\"\"\n",
    "    print(\"\\nğŸ“‹ Detailed Results Table:\")\n",
    "    \n",
    "    def highlight_results(val):\n",
    "        if val == 'YES':\n",
    "            return 'color: green; font-weight: bold'\n",
    "        elif val in ['NO', 'ERROR']:\n",
    "            return 'color: red; font-weight: bold'\n",
    "        elif val == 'UNKNOWN':\n",
    "            return 'color: orange; font-weight: bold'\n",
    "        return 'color: black'\n",
    "    \n",
    "    # Select key columns for display\n",
    "    display_cols = [\n",
    "        'test_id', 'method_used', 'rag_success',\n",
    "        'context_precision', 'faithfulness', 'answer_correctness', 'number_accuracy',\n",
    "        'response_length', 'chunk_count'\n",
    "    ]\n",
    "    \n",
    "    metric_cols = ['context_precision', 'faithfulness', 'answer_correctness', 'number_accuracy']\n",
    "    \n",
    "    styled_df = df[display_cols].style.applymap(\n",
    "        highlight_results, \n",
    "        subset=metric_cols\n",
    "    ).format({\n",
    "        'response_length': '{:.0f}',\n",
    "        'chunk_count': '{:.0f}'\n",
    "    })\n",
    "    \n",
    "    display(styled_df)\n",
    "\n",
    "# Run analysis\n",
    "analysis_summary = analyze_results(evaluation_results)\n",
    "create_detailed_results_table(evaluation_results)\n",
    "create_visualizations(evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a030ea1",
   "metadata": {},
   "source": [
    "## 10. Recommendations & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27370840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations(df: pd.DataFrame, analysis: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"Generate actionable recommendations based on results.\"\"\"\n",
    "    recommendations = []\n",
    "    \n",
    "    # RAG Success Rate\n",
    "    rag_success_rate = (df['rag_success'] == True).mean() * 100\n",
    "    if rag_success_rate < 80:\n",
    "        recommendations.append(\n",
    "            f\"ğŸ”§ **Fix Agent Orchestration**: Only {rag_success_rate:.1f}% of RAG calls succeeded. \"\n",
    "            f\"Check Gemini API integration and error handling.\"\n",
    "        )\n",
    "    \n",
    "    # Context Precision\n",
    "    context_precision = analysis['context_precision']['success_rate']\n",
    "    if context_precision < 70:\n",
    "        recommendations.append(\n",
    "            f\"ğŸ¯ **Improve Retrieval**: Context precision is {context_precision:.1f}%. \"\n",
    "            f\"Consider improving embedding quality or search relevance.\"\n",
    "        )\n",
    "    \n",
    "    # Faithfulness\n",
    "    faithfulness = analysis['faithfulness']['success_rate']\n",
    "    if faithfulness < 80:\n",
    "        recommendations.append(\n",
    "            f\"ğŸ“– **Reduce Hallucination**: Faithfulness is {faithfulness:.1f}%. \"\n",
    "            f\"Improve prompts to stay grounded in retrieved context.\"\n",
    "        )\n",
    "    \n",
    "    # Answer Correctness\n",
    "    correctness = analysis['answer_correctness']['success_rate']\n",
    "    if correctness < 60:\n",
    "        recommendations.append(\n",
    "            f\"âœ… **Improve Answer Quality**: Correctness is {correctness:.1f}%. \"\n",
    "            f\"Review golden dataset accuracy and model reasoning.\"\n",
    "        )\n",
    "    \n",
    "    # Number Accuracy\n",
    "    number_accuracy = analysis['number_accuracy']['success_rate']\n",
    "    if number_accuracy < 70:\n",
    "        recommendations.append(\n",
    "            f\"ğŸ”¢ **Fix Numerical Extraction**: Number accuracy is {number_accuracy:.1f}%. \"\n",
    "            f\"Improve financial data parsing and calculation accuracy.\"\n",
    "        )\n",
    "    \n",
    "    # Method fallback usage\n",
    "    retrieval_only_count = (df['method_used'] == 'retrieval_only').sum()\n",
    "    if retrieval_only_count > 0:\n",
    "        recommendations.append(\n",
    "            f\"ğŸ”„ **Agent Reliability**: {retrieval_only_count} tests fell back to retrieval-only. \"\n",
    "            f\"This indicates agent orchestration issues.\"\n",
    "        )\n",
    "    \n",
    "    # Error counts\n",
    "    total_errors = sum(s['error_count'] for s in analysis.values())\n",
    "    if total_errors > 0:\n",
    "        recommendations.append(\n",
    "            f\"âš ï¸ **Fix Evaluation Errors**: {total_errors} evaluation errors occurred. \"\n",
    "            f\"Check LLM judge connectivity and prompt formatting.\"\n",
    "        )\n",
    "    \n",
    "    # Data availability\n",
    "    if system_health['documents'] == 0:\n",
    "        recommendations.append(\n",
    "            f\"ğŸ“ **Upload Documents**: No documents found for test user. \"\n",
    "            f\"Upload financial documents to enable proper testing.\"\n",
    "        )\n",
    "    \n",
    "    # Positive feedback\n",
    "    overall_score = sum(s['success_rate'] for s in analysis.values()) / len(analysis)\n",
    "    if overall_score > 70:\n",
    "        recommendations.append(\n",
    "            f\"ğŸ‰ **Good Foundation**: Overall score of {overall_score:.1f}% shows the system has solid foundations.\"\n",
    "        )\n",
    "    \n",
    "    if not recommendations:\n",
    "        recommendations.append(\"âœ¨ **Excellent Performance**: All metrics are performing well!\")\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Generate and display recommendations\n",
    "recommendations = generate_recommendations(evaluation_results, analysis_summary)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ’¡ RECOMMENDATIONS & NEXT STEPS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"\\n{i}. {rec}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“‹ SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "overall_score = sum(s['success_rate'] for s in analysis_summary.values()) / len(analysis_summary)\n",
    "successful_rag = (evaluation_results['rag_success'] == True).sum()\n",
    "total_tests = len(evaluation_results)\n",
    "\n",
    "print(f\"ğŸ“Š **Evaluation Complete**: {total_tests} test cases processed\")\n",
    "print(f\"ğŸ¤– **RAG Success Rate**: {successful_rag}/{total_tests} ({(successful_rag/total_tests)*100:.1f}%)\")\n",
    "print(f\"ğŸ¯ **Overall Performance**: {overall_score:.1f}%\")\n",
    "print(f\"ğŸ’¾ **Results Saved**: {EVAL_RESULTS_PATH}\")\n",
    "\n",
    "# Best and worst performing metrics\n",
    "best_metric = max(analysis_summary.items(), key=lambda x: x[1]['success_rate'])\n",
    "worst_metric = min(analysis_summary.items(), key=lambda x: x[1]['success_rate'])\n",
    "\n",
    "print(f\"ğŸ† **Best Metric**: {best_metric[0].replace('_', ' ').title()} ({best_metric[1]['success_rate']:.1f}%)\")\n",
    "print(f\"âš ï¸ **Needs Work**: {worst_metric[0].replace('_', ' ').title()} ({worst_metric[1]['success_rate']:.1f}%)\")\n",
    "\n",
    "print(\"\\nâœ… **Evaluation harness completed successfully!**\")\n",
    "print(\"\\nğŸ”— **Next Steps**:\")\n",
    "print(\"   1. Address the recommendations above\")\n",
    "print(\"   2. Re-run evaluation after improvements\")\n",
    "print(\"   3. Compare results to track progress\")\n",
    "print(\"   4. Set up automated monitoring for regression detection\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
